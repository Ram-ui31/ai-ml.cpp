{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "84fe659d-deec-40cd-b687-726600e55ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hc/f67znl516xd8dpdwl3_517440000gn/T/ipykernel_94452/4209110604.py:5: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchaudio.transforms import MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "129e03ca-1f2c-4019-9f32-e8e5874df311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ramupadhyay/Documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "eec14020-df35-43ff-a35c-5e4245bdd20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "EPOCHS=\n",
    "LEARNING_RATE=0.0001\n",
    "SAMPLE_RATE=22050\n",
    "NUM_SAMPLES=22050*4\n",
    "N_MFCC=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b9b0e93e-7fa3-49a9-b286-6bc32a603423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transformation, target_sample_rate, num_samples, device):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        \n",
    "        self.audio_files = []\n",
    "        self.class_mapping = sorted(\n",
    "    [d for d in os.listdir(audio_dir) if not d.startswith(\".\")]\n",
    ")\n",
    "\n",
    "        for label_name in self.class_mapping:\n",
    "            class_folder = os.path.join(audio_dir, label_name)\n",
    "            for file in os.listdir(class_folder):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    self.audio_files.append((os.path.join(class_folder, file), label_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        audio_path, label_name = self.audio_files[index]\n",
    "        signal, sr = torchaudio.load(audio_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        label = self.class_mapping.index(label_name)\n",
    "        \n",
    "        return signal, label\n",
    "\n",
    "    # helper functions\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "524e3b9f-444f-4e77-907f-521b2a207ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transformation, target_sample_rate, num_samples, device):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        \n",
    "        self.audio_files = []\n",
    "        self.class_mapping = sorted(\n",
    "    [d for d in os.listdir(audio_dir) if not d.startswith(\".\")]\n",
    ")\n",
    "\n",
    "        for label_name in self.class_mapping:\n",
    "            class_folder = os.path.join(audio_dir, label_name)\n",
    "            for file in os.listdir(class_folder):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    self.audio_files.append((os.path.join(class_folder, file), label_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        audio_path, label_name = self.audio_files[index]\n",
    "        signal, sr = torchaudio.load(audio_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        label = self.class_mapping.index(label_name)\n",
    "        \n",
    "        return signal, label\n",
    "\n",
    "    \n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "bc447147-a787-4116-aafc-7fa360e76a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transformation, target_sample_rate, num_samples, device):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        \n",
    "        self.audio_files = []\n",
    "        self.class_mapping = sorted(\n",
    "    [d for d in os.listdir(audio_dir) if not d.startswith(\".\")]\n",
    ")\n",
    "\n",
    "        for label_name in self.class_mapping:\n",
    "            class_folder = os.path.join(audio_dir, label_name)\n",
    "            for file in os.listdir(class_folder):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    self.audio_files.append((os.path.join(class_folder, file), label_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        audio_path, label_name = self.audio_files[index]\n",
    "        signal, sr = torchaudio.load(audio_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        label = self.class_mapping.index(label_name)\n",
    "        \n",
    "        return signal, label\n",
    "\n",
    "    \n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6010ab2a-ed9f-4687-919c-7ae373b3863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(train_data,batch_size):\n",
    "    return DataLoader(train_data,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "60a06450-09a4-4ade-92a1-289509c8c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_epoch(model, data_loader, optimiser, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for input, target in data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        prediction = model(input)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    scheduler.step(avg_val_loss)\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d2528ee8-3782-4a54-b763-0401617bad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    \"\"\"Calculates loss and accuracy on a given dataloader.\"\"\"\n",
    "    model.eval() \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for input, target in dataloader:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            prediction = model(input)\n",
    "            \n",
    "            \n",
    "            total_loss += loss_fn(prediction, target).item()\n",
    "            \n",
    "           \n",
    "            _, predicted_class = torch.max(prediction, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted_class == target).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    model.train()\n",
    "    return avg_loss, accuracy\n",
    "def train(model, train_dataloader, validation_dataloader, loss_fn, optimizer, device, epochs, patience=5):\n",
    "    best_validation_loss = float('inf')\n",
    "    patience_counter = 0  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        current_train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        \n",
    "        for input, target in train_dataloader:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(input)\n",
    "            loss = loss_fn(prediction, target)\n",
    "            current_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = current_train_loss / len(train_dataloader)\n",
    "\n",
    "        \n",
    "        avg_val_loss, val_accuracy = evaluate(model, validation_dataloader, loss_fn, device)\n",
    "\n",
    "        print(f\"  Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "       \n",
    "        if avg_val_loss < best_validation_loss:\n",
    "            best_validation_loss = avg_val_loss\n",
    "            patience_counter = 0 \n",
    "            torch.save(model.state_dict(), \"fest_model_best.pth\")\n",
    "            print(\"  Saved Model - New Best Validation Loss!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  ⚠️ No improvement ({patience_counter}/{patience})\")\n",
    "\n",
    "        # --- Early stopping condition ---\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n⏹️ Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nFINISHED TRAINING!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "186609dc-892b-45f0-93a2-3a38bf7d2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 40, 44)): # Expects (Channels, MFCC_Bands, Time_Steps)\n",
    "        super().__init__()\n",
    "        self.Conv1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.Conv2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.Conv3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.Conv4=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(p=0.4) \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.rand(1, *input_shape)\n",
    "            x = self.Conv1(dummy_input)\n",
    "            x = self.Conv2(x)\n",
    "            x = self.Conv3(x)\n",
    "            x = self.Conv4(x)\n",
    "            flattened_size = self.flatten(x).shape[1]\n",
    "            \n",
    "        print(f\"INFO: Dynamically calculated flattened size is: {flattened_size}\")\n",
    "        self.linear = nn.Linear(6144, 5) \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.Conv1(input_data)\n",
    "        x = self.Conv2(x)\n",
    "        x = self.Conv3(x)\n",
    "        x = self.Conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b2a955b6-edaa-4428-8819-ff7bd75b2711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "INFO: Dynamically calculated flattened size is: 2048\n",
      "CNNNetwork(\n",
      "  (Conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (Conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (Conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (Conv4): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (linear): Linear(in_features=6144, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    print(\"Using CPU\")\n",
    "    device=\"cpu\"\n",
    "    mfcc_transformation = MFCC(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mfcc=N_MFCC,  \n",
    "    melkwargs={\n",
    "        \"n_fft\": 1024,\n",
    "        \"hop_length\": 512,\n",
    "        \"n_mels\": 64, \n",
    "    }\n",
    "    )\n",
    "    dataset = AudioDataset(\n",
    "    audio_dir=\"/Users/ramupadhyay/Desktop/train\",\n",
    "    transformation=mfcc_transformation,\n",
    "    target_sample_rate=SAMPLE_RATE,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    device=\"cpu\"\n",
    "    )\n",
    "    train_dataloader=create_data_loader(dataset,BATCH_SIZE)\n",
    "\n",
    "    TRAIN_RATIO = 0.85 \n",
    "    VALIDATION_RATIO = 0.15\n",
    "\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(TRAIN_RATIO * dataset_size)\n",
    "    validation_size = dataset_size - train_size\n",
    "    train_dataset, validation_dataset = torch.utils.data.random_split(\n",
    "    dataset, \n",
    "    [train_size, validation_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    cnn=CNNNetwork().to(device)\n",
    "    print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8a5c3290-1104-4585-93f3-be74e95c02c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "  Training Loss: 1.7082\n",
      "  Validation Loss: 1.2090, Validation Accuracy: 56.76%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 2\n",
      "  Training Loss: 1.2346\n",
      "  Validation Loss: 1.0681, Validation Accuracy: 61.20%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 3\n",
      "  Training Loss: 1.0564\n",
      "  Validation Loss: 0.9374, Validation Accuracy: 66.80%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 4\n",
      "  Training Loss: 0.9281\n",
      "  Validation Loss: 0.8623, Validation Accuracy: 69.31%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 5\n",
      "  Training Loss: 0.8386\n",
      "  Validation Loss: 0.7716, Validation Accuracy: 72.01%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 6\n",
      "  Training Loss: 0.7572\n",
      "  Validation Loss: 0.7286, Validation Accuracy: 74.71%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 7\n",
      "  Training Loss: 0.7011\n",
      "  Validation Loss: 0.6584, Validation Accuracy: 75.87%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 8\n",
      "  Training Loss: 0.6445\n",
      "  Validation Loss: 0.6192, Validation Accuracy: 77.99%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 9\n",
      "  Training Loss: 0.6120\n",
      "  Validation Loss: 0.5788, Validation Accuracy: 80.12%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 10\n",
      "  Training Loss: 0.5725\n",
      "  Validation Loss: 0.5591, Validation Accuracy: 81.85%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 11\n",
      "  Training Loss: 0.5476\n",
      "  Validation Loss: 0.5398, Validation Accuracy: 81.27%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 12\n",
      "  Training Loss: 0.5310\n",
      "  Validation Loss: 0.5870, Validation Accuracy: 79.73%\n",
      "---------------------------------\n",
      "Epoch 13\n",
      "  Training Loss: 0.5130\n",
      "  Validation Loss: 0.4903, Validation Accuracy: 83.20%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 14\n",
      "  Training Loss: 0.4822\n",
      "  Validation Loss: 0.4716, Validation Accuracy: 81.85%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 15\n",
      "  Training Loss: 0.4617\n",
      "  Validation Loss: 0.4848, Validation Accuracy: 82.82%\n",
      "---------------------------------\n",
      "Epoch 16\n",
      "  Training Loss: 0.4411\n",
      "  Validation Loss: 0.4754, Validation Accuracy: 84.56%\n",
      "---------------------------------\n",
      "Epoch 17\n",
      "  Training Loss: 0.4274\n",
      "  Validation Loss: 0.4449, Validation Accuracy: 84.75%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 18\n",
      "  Training Loss: 0.4010\n",
      "  Validation Loss: 0.4492, Validation Accuracy: 82.24%\n",
      "---------------------------------\n",
      "Epoch 19\n",
      "  Training Loss: 0.3732\n",
      "  Validation Loss: 0.4228, Validation Accuracy: 85.91%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 20\n",
      "  Training Loss: 0.3503\n",
      "  Validation Loss: 0.4012, Validation Accuracy: 87.07%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 21\n",
      "  Training Loss: 0.3362\n",
      "  Validation Loss: 0.3790, Validation Accuracy: 85.91%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 22\n",
      "  Training Loss: 0.3219\n",
      "  Validation Loss: 0.3677, Validation Accuracy: 87.26%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 23\n",
      "  Training Loss: 0.3116\n",
      "  Validation Loss: 0.3679, Validation Accuracy: 86.29%\n",
      "---------------------------------\n",
      "Epoch 24\n",
      "  Training Loss: 0.3033\n",
      "  Validation Loss: 0.3729, Validation Accuracy: 87.84%\n",
      "---------------------------------\n",
      "Epoch 25\n",
      "  Training Loss: 0.2893\n",
      "  Validation Loss: 0.3470, Validation Accuracy: 89.38%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 26\n",
      "  Training Loss: 0.2773\n",
      "  Validation Loss: 0.3704, Validation Accuracy: 87.07%\n",
      "---------------------------------\n",
      "Epoch 27\n",
      "  Training Loss: 0.2725\n",
      "  Validation Loss: 0.3062, Validation Accuracy: 89.19%\n",
      "---------------------------------\n",
      "  ✅ Saved Model - New Best Validation Loss!\n",
      "Epoch 28\n",
      "  Training Loss: 0.2454\n",
      "  Validation Loss: 0.3218, Validation Accuracy: 89.19%\n",
      "---------------------------------\n",
      "Epoch 29\n",
      "  Training Loss: 0.2506\n",
      "  Validation Loss: 0.3111, Validation Accuracy: 89.38%\n",
      "---------------------------------\n",
      "Epoch 30\n",
      "  Training Loss: 0.2413\n",
      "  Validation Loss: 0.3117, Validation Accuracy: 89.58%\n",
      "---------------------------------\n",
      "FINISHED TRAINING!\n"
     ]
    }
   ],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimiser=torch.optim.Adam(cnn.parameters(),lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimiser, \n",
    "    mode='min', \n",
    "    factor=0.5, # Reduce LR by half\n",
    "    patience=3 # If validation loss doesn't improve for 3 epochs\n",
    ")\n",
    "train(cnn, train_dataloader, validation_dataloader, loss_fn, optimiser, device, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7eec0baf-77b6-48b0-ac15-a7c995091791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping=[\n",
    "    \"dog_bark\",\n",
    "    \"drilling\",\n",
    "    \"engine_idling\",\n",
    "    \"siren\",\n",
    "    \"street_music\"\n",
    "]\n",
    "def predict(model, input, target, class_mapping):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input)\n",
    "        predicted_indices = torch.argmax(predictions, dim=1)\n",
    "        predicted_labels = [class_mapping[i.item()] for i in predicted_indices]\n",
    "\n",
    "        if target is not None:\n",
    "            expected = [class_mapping[t] for t in target]\n",
    "            return predicted_labels, expected\n",
    "        else:\n",
    "            return predicted_labels\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transformation, target_sample_rate, num_samples, device):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.files = [f for f in os.listdir(audio_dir) if f.endswith(\".wav\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.files[index]\n",
    "        file_path = os.path.join(self.audio_dir, file_name)\n",
    "        signal, sr = torchaudio.load(file_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, file_name\n",
    "\n",
    "    # helper functions (same as before)\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a56ceb04-9629-4748-becc-e495e4cc7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(\n",
    "    audio_dir=\"/Users/ramupadhyay/Desktop/test\",\n",
    "    transformation=mfcc_transformation,\n",
    "    target_sample_rate=SAMPLE_RATE,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8e518a95-ec18-4df2-97f7-8a18dd06963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ram14_submission.csv created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cnn.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, file_names in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        predictions = cnn(inputs)\n",
    "        predicted_indices = predictions.argmax(dim=1)\n",
    "        for fname, pred_idx in zip(file_names, predicted_indices):\n",
    "            label = class_mapping[pred_idx.item()]\n",
    "            results.append((fname, label))\n",
    "\n",
    "\n",
    "# create submission DataFrame\n",
    "submission_df = pd.DataFrame(results, columns=[\"ID\", \"Class\"])\n",
    "submission_df.to_csv(\"ram16_submission.csv\", index=False)\n",
    "print(\"ram16_submission.csv created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6df418-95cc-4868-9988-80214c3f1a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (audio)",
   "language": "python",
   "name": "audioenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
